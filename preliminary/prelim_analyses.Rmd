---
title: "Preliminary Analyses for *Race & Nation on Twitter: Romanian & Bulgarian Migration to the UK*"
author: "Roushdat Elaheebocus, Justin Murphy, Jessica Ogden, and Bindi Shah"
output: html_document
keep_md: true
---
```{r, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
setwd("~/Dropbox/gh_projects/migration_tweets")
require(jsonlite)
require(lubridate)
require(ggplot2)
require(twitteR)
require(stringr)
library(igraph)
require(tm)
#require(Snowball)
require(RColorBrewer)

# df <- fromJSON("output.json", flatten=TRUE)
# df<-df[complete.cases(df["user.screen_name"]),]
# save(df, file="cleaned_data/complete.RData")

load("cleaned_data/complete.RData")
```

The code repository for this project is at [https://github.com/jmrphy/migration_tweets](https://github.com/jmrphy/migration_tweets). You can send questions or comments to Justin Murphy at <a href="mailto:j.murphy@soton.ac.uk">j.murphy@soton.ac.uk</a> or <a href="http://twitter.com/jmrphy">@jmrphy</a>.

This document is a preliminary analysis of every Twitter status update created between October 1, 2013 and March 1, 2014 which refers to immigration and contains any of the following terms: Bulgaria/Bulgarian, Romania/Romanian, England, UK, Britain. The sample contains a total of `r length(df$created_at)` tweets.

```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.cap="Tweets on UK-Romanian-Bulgarian Migration, October 2013-February 2014"}

# summary(as.factor(df$created_at))

df$time<-as.POSIXct(dmy_hms(substr(df$created_at, 6, 25))) # Doesnt work, need to wrangle date

# random<-df[sample(nrow(df), 1000), ]
# random<-subset(random, select=c("user.screen_name", "user.description", "created_at", "text", "time"))
# random<-random[order(random$time),]
# write.csv(random, file="cleaned_data/random_sample.csv")


time.series<-ggplot(data=df, aes(x=time)) +
  geom_bar(aes(fill=..count..)) +
  theme_bw() +
  labs(y="Tweets", x="Month", title="Tweets about Romanian and Bulgarian , 10/2013 - 02/2014")

time.series
```

```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.cap="Most Frequent Tweeters"}
counts<-as.data.frame(table(df$user.screen_name))
counts<-subset(counts, Freq>85)
rownames(counts)<-NULL
counts$Var1<-factor(counts$Var1)

frequencies<-ggplot(data=counts, aes(x=reorder(Var1, Freq), y=Freq)) +
  geom_point(stat="identity") +
  labs(x="Username", y="Number of Tweets", title="Most Frequent Tweeters") +
  theme_bw() +
  coord_flip()

frequencies
```

```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.cap="Most Retweeted Users (Total Retweets Received)"}

df$text=sapply(df$text,function(row) iconv(row,to='UTF-8')) #remove odd characters
trim <- function (x) sub('@','',x) # remove @ symbol from user names
# extract who's been retweeted by whom
df$rt=sapply(df$text,function(tweet) trim(str_match(tweet,"^RT (@[[:alnum:]_]*)")[2]))
totalrts<-sum(!is.na(df$rt)) # see how many tweets are retweets
# sum(!is.na(df$rt))/length(df$rt) # the ratio of retweets to tweets
countRT<-table(df$rt)
countRT<-sort(countRT)
countRT.subset=subset(countRT,countRT>=4) # subset those RTâ€™d more than 5 times
countRT.subset.df<-data.frame(people=as.factor(unlist(dimnames(countRT.subset))),RT_count=as.numeric(unlist(countRT.subset)))

retweets<-ggplot(countRT.subset.df, aes(reorder(people,RT_count),RT_count)) +
  xlab("Username") + ylab("Number of Retweets Received") +
  ggtitle("Most Retweeted Users") +
  geom_point(stat="identity") + coord_flip() + theme_bw()

retweets
```

There is a total of `r totalrts` retweets in the sample. I'm pretty sure this is much much fewer than other studies of more specific conversations, suggesting there is a lot of disconnected tweeting.

```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.cap="Most Retweeted Users (As a Share of Total Tweets)"}
t<-as.data.frame(table(df$user.screen_name)) # make table with counts of tweets per person
rt<-as.data.frame(table(df$rt)) # make table with counts of retweets per person
t.rt<-merge(t,rt,by="Var1") # combine tweet count and retweet count per person
t.rt["ratio"]<-t.rt$Freq.y / t.rt$Freq.x # creates new col and adds ratio tweet/retweet
sort.t.rt<-t.rt[order(t.rt$ratio),] # sort it to put names in order by ratio
sort.t.rt.subset<-subset(sort.t.rt,sort.t.rt$Freq.y>3) # exclude those with less than 5 tweets
sort.t.rt.subset.drop<-droplevels(sort.t.rt.subset) # drop unused levels that got in there

ratios<-ggplot(sort.t.rt.subset, aes(reorder(Var1,ratio),ratio)) +
  xlab("Username") + ylab("Ratio of Retweets to Total Tweets") +
  ggtitle("Most Retweeted Users") +
  geom_point(stat="identity") + coord_flip() + theme_bw()

ratios
```



```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.cap="Top links"}
df$link=sapply(df$text,function(tweet) str_extract(tweet,("http[[:print:]]+"))) # creates new field and extracts the links contained in the tweet
df$link=sapply(df$text,function(tweet) str_extract(tweet,"http[[:print:]]{16}")) # limits to just 16 characters after http so I just get the shortened link. They are all shortened, so this is fine, but there might be a better way using regex.
countlink<-table(df$link) # get frequencies of each link
countlink<-sort(countlink) # sort them
countlink<-data.frame(table(na.omit((df$link))))
countlink<-subset(countlink,countlink$Freq>40) # exclude those with 300 tweets or less

links<-ggplot(countlink, aes(reorder(Var1, Freq), Freq)) +
  geom_bar(stat="identity") + coord_flip() + theme_bw() +
  xlab("Link") + ylab("Frequency") +
  labs(title="Most Shared Links")

links
```

```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.height=11, fig.width=8, fig.cap="Whole Network", eval=FALSE}
setwd("~/Dropbox/gh_projects/migration_tweets/cleaned_data")
el.df<-read.csv("migration_el.csv") # read in edgelist
setwd("~/Dropbox/gh_projects/migration_tweets")
el.df<-el.df[,2:3]
rt_graph2 = graph.edgelist(as.matrix(el.df), directed=TRUE)

# Make and plot graph
rt_graph2 = delete.vertices(rt_graph2, V(rt_graph2)[ degree(rt_graph2)==0 ])

V(rt_graph2)$color[betweenness(rt_graph2)>2000] =  rgb(1,0,0,1)
V(rt_graph2)$color[evcent(rt_graph2)$vector>.3] =  rgb(0,1,0,1)

V(rt_graph2)$size = 2 
V(rt_graph2)$label[evcent(rt_graph2)$vector>.3] = V(rt_graph2)$name[evcent(rt_graph2)$vector>.3]
V(rt_graph2)$label[betweenness(rt_graph2)>2000] = V(rt_graph2)$name[betweenness(rt_graph2)>2000]
V(rt_graph2)$label.cex = 1.2

E(rt_graph2)$width = .3
E(rt_graph2)$color = rgb(.5,.5,0,.1)

set.seed(4074)
par(bg="white", mar=c(1,1,1,1))
plot.igraph(rt_graph2, layout=layout.fruchterman.reingold, vertex.label.color= "white", main="Whole Retweet Network")

fc <- walktrap.community(rt_graph2)

colors <- rainbow(max(membership(fc)))
plot(rt_graph2,vertex.color=colors[membership(fc)], 
     layout=layout.fruchterman.reingold)
```

Eigenvector centrality refers to the degree to which nodes are connected to other highly connected nodes. This is a standard overall indicator of influence or network power. Betweenness centrality refers to the degree to which a node connects the other nodes of the network. Nodes with high betweeness centrality are "brokers," not necessarily influential in a direct sense but powerful because they provide certain clusters with access to other clusters.

```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.height=8, fig.width=8, fig.cap="Key Actors", eval=FALSE}
cent<-data.frame(bet=betweenness(rt_graph2),eig=evcent(rt_graph2)$vector)
res<-lm(eig~bet,data=cent)$residuals
cent<-transform(cent,res=res)
set.seed(4074)
key.accounts<-ggplot(cent,aes(x=bet,y=eig,
                              label=rownames(cent),colour=res)) +
  xlab("Betweenness Centrality") + 
  ylab("Eigenvector Centrality") +
  labs(title="Key Twitter Accounts") +
  theme_bw() +
  geom_text(size=6, position = position_jitter(width = 300, height=.15))

key.accounts
```

# Interactive Networks by Month

For each month, the first network (blue) reflects the 99th percentile of most retweeted usernames, while the second network (red) reflects a 3% random sample of the smallest conversations (between 2 and 4 retweets).

## October
```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.height=11, fig.width=8, results='asis'}

require(networkD3)

setwd("~/Dropbox/gh_projects/migration_tweets/cleaned_data")
el.df1<-read.csv("migration_el1.csv") # read in edgelist
el.df2<-read.csv("migration_el2.csv") # read in edgelist
el.df3<-read.csv("migration_el3.csv") # read in edgelist
el.df4<-read.csv("migration_el4.csv") # read in edgelist
el.df5<-read.csv("migration_el5.csv") # read in edgelist


setwd("~/Dropbox/gh_projects/migration_tweets")
el.df1<-el.df1[,2:3]
el.df2<-el.df2[,2:3]
el.df3<-el.df3[,2:3]
el.df4<-el.df4[,2:3]
el.df5<-el.df5[,2:3]

# October network graphs

post.table<-table(el.df1$who_post)

simpleNetwork(el.df1[el.df1$who_post %in% names(post.table[post.table > quantile(post.table, .99)]), ], Source="who_post", Target="who_retweet", charge=-50, opacity=.5,
              width=900,
              height=800,
              fontSize = 12,
              linkDistance=40)

simpleNetwork(el.df1[el.df1$who_post %in% sample(names(post.table[post.table <= 4 & post.table >=2]), length(el.df1[,1])/33),], charge=-40, opacity=.5, Target="who_post", Source="who_retweet",
              width=900,
              height=800,
              fontSize = 12,
              linkDistance=25,
              nodeColour="red",
              nodeClickColour="black")

```

## November
```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.height=11, fig.width=8, results='asis'}
post.table<-table(el.df2$who_post)

simpleNetwork(el.df2[el.df2$who_post %in% names(post.table[post.table > quantile(post.table, .99)]), ], Source="who_post", Target="who_retweet", charge=-50, opacity=.5,
              width=900,
              height=800,
              fontSize = 12,
              linkDistance=40)

simpleNetwork(el.df2[el.df2$who_post %in% sample(names(post.table[post.table <= 4 & post.table >=2]), length(el.df2[,1])/33),], charge=-40, opacity=.5, Target="who_post", Source="who_retweet",
              width=900,
              height=800,
              fontSize = 12,
              linkDistance=25,
              nodeColour="red",
              nodeClickColour="black")

```

## December

```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.height=11, fig.width=8, results='asis'}
post.table<-table(el.df3$who_post)

simpleNetwork(el.df3[el.df3$who_post %in% names(post.table[post.table > quantile(post.table, .99)]), ], Source="who_post", Target="who_retweet", charge=-50, opacity=.5,
              width=900,
              height=800,
              fontSize = 12,
              linkDistance=40)

simpleNetwork(el.df3[el.df3$who_post %in% sample(names(post.table[post.table <= 4 & post.table >=2]), length(el.df3[,1])/33),], charge=-40, opacity=.5, Target="who_post", Source="who_retweet",
              width=900,
              height=800,
              fontSize = 12,
              linkDistance=25,
              nodeColour="red",
              nodeClickColour="black")

```

## January

```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.height=11, fig.width=8, results='asis'}
post.table<-table(el.df4$who_post)

simpleNetwork(el.df4[el.df4$who_post %in% names(post.table[post.table > quantile(post.table, .99)]), ], Source="who_post", Target="who_retweet", charge=-50, opacity=.5,
              width=900,
              height=800,
              fontSize = 12,
              linkDistance=40)

simpleNetwork(el.df4[el.df4$who_post %in% sample(names(post.table[post.table <= 4 & post.table >=2]), length(el.df4[,1])/33),], charge=-40, opacity=.5, Target="who_post", Source="who_retweet",
              width=900,
              height=800,
              fontSize = 12,
              linkDistance=25,
              nodeColour="red",
              nodeClickColour="black")

```

## February

```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.height=11, fig.width=8, results='asis'}
post.table<-table(el.df5$who_post)

simpleNetwork(el.df5[el.df5$who_post %in% names(post.table[post.table > quantile(post.table, .99)]), ], Source="who_post", Target="who_retweet", charge=-50, opacity=.5,
              width=900,
              height=800,
              fontSize = 12,
              linkDistance=40)

simpleNetwork(el.df5[el.df5$who_post %in% sample(names(post.table[post.table <= 4 & post.table >=2]), length(el.df5[,1])/33),], charge=-40, opacity=.5, Target="who_post", Source="who_retweet",
              width=900,
              height=800,
              fontSize = 12,
              linkDistance=25,
              nodeColour="red",
              nodeClickColour="black")

```
Below are the most central accounts for each month. The first group of five list those with the highest eigenvector centrality for each month (in order). The second group of five list those with the highest betweenness centrality for each month (in order). 


```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.height=11, fig.width=8, results='hide', eval=FALSE}

#################################################################################
### Top words #########
#################################################################################

df$cleantext<-gsub('http.* *', ' ', df$text)
df$cleantext<-str_replace_all(df$cleantext, "[^[:alnum:]]", " ")
corp<-Corpus(VectorSource(df$cleantext))
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, stripWhitespace)

my_stopwords <- c(stopwords('english'), paste(unique(df$screenName)))
corp <- tm_map(corp, removeWords, my_stopwords)


corp <- tm_map(corp, tolower)

corp.full<-corp

corp <- tm_map(corp, stemDocument, language = "english") #reduce all English words to their roots

# other_stop_words<-c("psa", "polit", "rt", "panel", "amp", "politicalspik", "angeliawilson",
#                     "politicsir", "peterjohn", "the")
# corp <- tm_map(corp, removeWords, other_stop_words)   # remove specific word (the hashtag of interest)

dtm <-DocumentTermMatrix(corp) # make a matrix of each document by every single term
dtm <- removeSparseTerms(dtm, 0.99)

terms<-as.data.frame(sort(colSums(inspect(dtm)), decreasing=TRUE))
```


```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.height=11, fig.width=8, eval=FALSE}
terms$Term <- rownames(terms)
names(terms)<-c("Frequency", "Term")
rownames(terms) = NULL
terms[1:25,1:2]

```

```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.height=11, fig.width=8, eval=FALSE}

######## Sentiment
setwd("~/Dropbox/gh_projects/migration_tweets/preliminary")
hu.liu.pos=scan("positive-words.txt",what='character',comment.char=';') #load +ve sentiment word list
hu.liu.neg=scan("negative-words.txt",what='character',comment.char=';') #load -ve sentiment word list
pos.words=c(hu.liu.pos)
neg.words=c(hu.liu.neg)

score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
  require(plyr)
  require(stringr)
  
  # we got a vector of sentences. plyr will handle a list
  # or a vector as an "l" for us
  # we want a simple array ("a") of scores back, so we use
  # "l" + "a" + "ply" = "laply":
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    
    # clean up sentences with R's regex-driven global substitute, gsub():
    sentence = gsub('[[:punct:]]', '', sentence)
    sentence = gsub('[[:cntrl:]]', '', sentence)
    sentence = gsub('\\d+', '', sentence)
    # and convert to lower case:
    sentence = tolower(sentence)
    
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, '\\s+')
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    
    # compare our words to the dictionaries of positive & negative terms
    pos.matches = match(words, pos.words)
    neg.matches = match(words, neg.words)
    
    # match() returns the position of the matched term or NA
    # we just want a TRUE/FALSE:
    pos.matches = !is.na(pos.matches)
    neg.matches = !is.na(neg.matches)
    
    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
    score = sum(pos.matches) - sum(neg.matches)
    
    return(score)
  }, pos.words, neg.words, .progress=.progress )
  
  scores.df = data.frame(score=scores, text=sentences)
  return(scores.df)
}

corp.full<-unlist(corp.full)

corp.scores<-score.sentiment(corp.full,pos.words,neg.words) # get scores for the tweet text 

ggplot(corp.scores, aes(x=score)) + geom_histogram(binwidth=1) + xlab("Sentiment score") + ylab("Frequency") + theme_bw() + ggtitle("The Distribution of Sentiment")

corp.pos<-subset(corp.scores,corp.scores$score>=4) # get tweets with only very +ve scores
corp.neg<-subset(corp.scores,corp.scores$score<=-4) # get tweets with only very -ve scores

# mean(corp.scores$score)
```

## Random Examples of Most Negative Tweets

```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.height=11, fig.width=8, eval=FALSE}
corp.neg$text[1:25]
```

## Random Examples of Most Positive Tweets

```{r, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.height=11, fig.width=8, eval=FALSE}
corp.pos$text[1:25]
```